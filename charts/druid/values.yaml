# Analytics stack configuration - Apache Druid
# This Chart is designed to be deployed as a SubChart of the
# Broadcom API Developer Portal

# Global values
global:
  portalRepository: caapim/
  pullSecret: broadcom-apim
  # these default to mysql settings above, change if connecting to an external database.
# setupDemoDatabase: true
  databaseType: mysql
  databaseHost: mysql
  databasePort: 3306
  databaseUseSSL: true
  databaseRequireSSL: false
  databaseUsername: admin
  databaseSecret:
  subdomainPrefix: dev-portal
  legacyHostnames: false
  legacyDatabaseNames: false
  # databasePassword: 7layer
  s3BucketName: api-metrics
  imageName: chrislusf/seaweedfs
  restartPolicy: Always
  loggingLevel: 1
  enableReplication: false
  #  replication type is XYZ:
  # X number of replica in other data centers
  # Y number of replica in other racks in the same data center
  # Z number of replica in other servers in the same rack
  replicationPlacment: "001"
  
  extraEnvironmentVars:
    WEED_CLUSTER_DEFAULT: "sw"
    WEED_CLUSTER_SW_MASTER: "seaweedfs-master:9333"
    WEED_CLUSTER_SW_FILER: "seaweedfs-filer-client:8888"

forceRedeploy: false

image:
  zookeeper: zookeeper:5.0
  broker: druid:5.0
  coordinator: druid:5.0
  middlemanager: druid:5.0
  minio: minio:5.0
  historical: druid:5.0
  kafka: kafka:5.0
  ingestion: ingestion-server:5.0
  seaweedfs: chrislusf/seaweedfs:2.76

persistence:
  storage:
    historical: 50Gi
    minio: 40Gi
    kafka: 10Gi
    zookeeper: 10Gi

# Service Account for Druid services
serviceAccount:
  create: true
# name:

minio:
  enable: true
  auth:
    secretName: minio-secret
    # Auto-generated minio-auth
  # access_key:
  # secret_key:
# The minio bucket that holds historical data for your portal.
# By default this is apim-data, if using AWS, GCP, Azure, this will need to match a bucket you have access to.
  bucketName: api-metrics
# cloudStorage: false
  s3gateway:
    enabled: false
    serviceEndpoint: ""
    accessKey: ""
    secretKey: ""
  gcsgateway:
    enabled: false
    # credential json file of service account key
    gcsKeyJson: ""
    # Google cloud project-id
    projectId: ""
  azuregateway:
    enabled: false
  replicaCount: 1
  port: 9000
  image:
    pullPolicy: IfNotPresent
  resources:
    requests: {}
    # memory: 256Mi
    limits: {}
    # memory: 256Mi
# nodeSelector: {}
# affinity:
  

zookeeper:
  hostname: zookeeper
  port: 2181
  replicaCount: 1
  image:
    pullPolicy: IfNotPresent
  resources:
    limits: {}
    # memory: 256Mi
    requests: {}
    # memory: 256Mi
# nodeSelector: {}
# affinity:

middlemanager:
  replicaCount: 1
  loglevel: WARN
  indexer:
    startPort: 8100
    endPort: 65535
  image:
    pullPolicy: IfNotPresent
  resources:
    requests: {}
    # memory: 4Gi
    limits: {}
    # memory: 4Gi
# nodeSelector: {}
# affinity:

broker:
  replicaCount: 1
  loglevel: WARN
  image:
    pullPolicy: IfNotPresent
  resources:
    requests: {}
    # memory: 1Gi
    limits: {}
    # memory: 2Gi
# nodeSelector: {}
# affinity:

coordinator:
  hostname: coordinator
  loglevel: WARN
  port: 8081
  replicaCount: 1
  image:
    pullPolicy: IfNotPresent
  resources:
    requests: {}     
    # memory: 512Mi
    limits: {}
    # memory: 512Mi
# nodeSelector: {}
# affinity:

historical:
  replicaCount: 1
  loglevel: WARN
  image:
    pullPolicy: IfNotPresent
  resources:
    requests: {}
    # memory: 2Gi
    limits: {}
    # memory: 2Gi
# nodeSelector: {}
# affinity:

kafka:
  replicaCount: 1
  image:
    pullPolicy: IfNotPresent
  resources:
    requests: {}
    # cpu: 100m
    # memory: 750Mi
    limits: {}
    # cpu: 1000m
    # memory: 1.5Gi
# nodeSelector: {}
# affinity:

ingestion:
  replicaCount: 1
  image:
    pullPolicy: IfNotPresent
  config:
    partitionCount: 1
  portName: ingestion-server
  resources:
    requests: {}
    # cpu: 100m
    # memory: 512Mi
    limits: {}
    # cpu: 1000m
    # memory: 1.5Gi
# nodeSelector: {}
# affinity:


master:
  enabled: true
  imageOverride: chrislusf/seaweedfs:2.76
  replicas: 1
  port: 9333
  grpcPort: 19333
  ipBind: "0.0.0.0"
  volumePreallocate: false
  #Master stops directing writes to oversized volumes
  volumeSizeLimitMB: 30000
  loggingOverrideLevel: null
  #number of seconds between heartbeats, default 5
  pulseSeconds: null
  #threshold to vacuum and reclaim spaces, default 0.3 (30%)
  garbageThreshold: null
  #Prometheus push interval in seconds, default 15
  metricsIntervalSec: 15
  #  replication type is XYZ:
  # X number of replica in other data centers
  # Y number of replica in other racks in the same data center
  # Z number of replica in other servers in the same rack
  defaultReplication: "000"

  # Disable http request, only gRpc operations are allowed
  disableHttp: false

  extraVolumes: ""
  extraVolumeMounts: ""

  # storage and storageClass are the settings for configuring stateful
  # storage for the master pods. storage should be set to the disk size of
  # the attached volume. storageClass is the class of storage which defaults
  # to null (the Kube cluster will pick the default).
  storage: 25Gi
  storageClass: null

  # Resource requests, limits, etc. for the master cluster placement. This
  # should map directly to the value of the resources field for a PodSpec,
  # formatted as a multi-line string. By default no direct resource request
  # is made.
  resources: null

  # updatePartition is used to control a careful rolling update of SeaweedFS
  # masters.
  updatePartition: 0

  extraEnvironmentVars:
    WEED_MASTER_VOLUME_GROWTH_COPY_1: 7
    WEED_MASTER_VOLUME_GROWTH_COPY_2: 6
    WEED_MASTER_VOLUME_GROWTH_COPY_3: 3
    WEED_MASTER_VOLUME_GROWTH_COPY_OTHER: 1

volume:
  enabled: true
  imageOverride: chrislusf/seaweedfs:2.76
  port: 8080
  grpcPort: 18080
  metricsPort: 9327
  ipBind: "0.0.0.0"
  replicas: 1
  loggingOverrideLevel: null
  # number of seconds between heartbeats, must be smaller than or equal to the master's setting
  pulseSeconds: null
  # Choose [memory|leveldb|leveldbMedium|leveldbLarge] mode for memory~performance balance., default memory
  index: null
  # limit file size to avoid out of memory, default 256mb
  fileSizeLimitMB: null
  # minimum free disk space(in percents). If free disk space lower this value - all volumes marks as ReadOnly
  minFreeSpacePercent: 7

  #can use ANY storage-class , example with local-path-provisner
  data:
    type: "persistentVolumeClaim"
    size: "10Gi"
    storageClass: "standard"
  # data:
  #   type: "hostPath"
  #   size: ""
  #   storageClass: ""
  idx:
    type: "persistentVolumeClaim"
    size: "1Gi"
    storageClass: "standard"

  logs:
    type: "persistentVolumeClaim"
    size: "1Gi"
    storageClass: "standard"

  # limit background compaction or copying speed in mega bytes per second
  compactionMBps: "50"

  # Directories to store data files. dir[,dir]... (default "/tmp")
  dir: "/data"
  # Directories to store index files. dir[,dir]... (default is the same as "dir")
  dir_idx: null

  # Maximum numbers of volumes, count[,count]...
  # If set to zero on non-windows OS, the limit will be auto configured. (default "7")
  maxVolumes: "0"

  # Volume server's rack name
  rack: null

  # Volume server's data center name
  dataCenter: null

  # Redirect moved or non-local volumes. (default proxy)
  readMode: proxy

  # Comma separated Ip addresses having write permission. No limit if empty.
  whiteList: null

  # Adjust jpg orientation when uploading.
  imagesFixOrientation: false

  extraVolumeMounts: null

filer:
  enabled: true
  imageOverride: chrislusf/seaweedfs:2.76
  replicas: 1
  port: 8888
  grpcPort: 18888
  metricsPort: 9327
  loggingOverrideLevel: null
  #  replication type is XYZ:
  # X number of replica in other data centers
  # Y number of replica in other racks in the same data center
  # Z number of replica in other servers in the same rack
  defaultReplicaPlacement: "000"

  # split files larger than the limit, default 32
  maxMB: null
  # encrypt data on volume servers
  encryptVolumeData: false
  # enable peers sync metadata, for leveldb (localdb for filer but with sync across)
  enable_peers: false

  # Whether proxy or redirect to volume server during file GET request
  redirectOnRead: false

  # Limit sub dir listing size (default 100000)
  dirListLimit: 100000

  # Turn off directory listing
  disableDirListing: false

  # Disable http request, only gRpc operations are allowed
  disableHttp: false

  # storage and storageClass are the settings for configuring stateful
  # storage for the master pods. storage should be set to the disk size of
  # the attached volume. storageClass is the class of storage which defaults
  # to null (the Kube cluster will pick the default).
  storage: 25Gi
  storageClass: standard

  extraVolumes: ""
  extraVolumeMounts: ""

  # updatePartition is used to control a careful rolling update of SeaweedFS
  # masters.
  updatePartition: 0

  # Resource requests, limits, etc. for the server cluster placement. This
  # should map directly to the value of the resources field for a PodSpec,
  # formatted as a multi-line string. By default no direct resource request
  # is made.
  resources: null

  # Toleration Settings for server pods
  # This should be a multi-line string matching the Toleration array
  # in a PodSpec.
  tolerations: ""

  # used to assign priority to server pods
  # ref: https://kubernetes.io/docs/concepts/configuration/pod-priority-preemption/
  priorityClassName: ""

  # extraEnvVars is a list of extra enviroment variables to set with the stateful set.
  extraEnvironmentVars:
    WEED_MYSQL_ENABLED: "true"
    # WEED_MYSQL_HOSTNAME: "rancher-mysql-01.apim.broadcom.net"
    # WEED_MYSQL_PORT: "3306"
    # WEED_MYSQL_DATABASE: "seaweedfs"
    WEED_MYSQL_CONNECTION_MAX_IDLE: "5"
    WEED_MYSQL_CONNECTION_MAX_OPEN: "75"
    # "refresh" connection every 10 minutes, eliminating mysql closing "old" connections
    WEED_MYSQL_CONNECTION_MAX_LIFETIME_SECONDS: "600"
    # enable usage of memsql as filer backend
    WEED_MYSQL_INTERPOLATEPARAMS: "true"
    WEED_LEVELDB2_ENABLED: "false"
    # with http DELETE, by default the filer would check whether a folder is empty.
    # recursive_delete will delete all sub folders and files, similar to "rm -Rf"
    WEED_FILER_OPTIONS_RECURSIVE_DELETE: "false"
    # directories under this folder will be automatically creating a separate bucket
    WEED_FILER_BUCKETS_FOLDER: "/buckets"

  s3:
    enabled: true
    port: 8333
    #allow empty folders
    allowEmptyFolder: false
    # Suffix of the host name, {bucket}.{domainName}
    domainName: ""
    # enable user & permission to s3 (need to inject to all services)
    enableAuth: false
    skipAuthSecretCreation: false

s3:
  enabled: false
  imageOverride: chrislusf/seaweedfs:2.76
  replicas: 1
  port: 8333
  metricsPort: 9327
  loggingOverrideLevel: null
  #allow empty folders
  allowEmptyFolder: true
  # enable user & permission to s3 (need to inject to all services)
  enableAuth: false
  skipAuthSecretCreation: false

  # Suffix of the host name, {bucket}.{domainName}
  domainName: ""

  extraVolumes: ""
  extraVolumeMounts: ""

  # Resource requests, limits, etc. for the server cluster placement. This
  # should map directly to the value of the resources field for a PodSpec,
  # formatted as a multi-line string. By default no direct resource request
  # is made.
  resources: null

  # Toleration Settings for server pods
  # This should be a multi-line string matching the Toleration array
  # in a PodSpec.
  tolerations: ""

  # nodeSelector labels for server pod assignment, formatted as a muli-line string.
  # ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#nodeselector
  # Example:
  # nodeSelector: |
  #   beta.kubernetes.io/arch: amd64
  # nodeSelector: |
  #   sw-backend: "true"

  # used to assign priority to server pods
  # ref: https://kubernetes.io/docs/concepts/configuration/pod-priority-preemption/
  priorityClassName: ""

  logs:
    type: "hostPath"
    size: ""
    storageClass: ""

cronjob:
  enabled: true
  imageOverride: chrislusf/seaweedfs:2.76
  master: "seaweedfs-master:9333"
  filer: "seaweedfs-filer-client:8888"
  tolerations: ""
  # nodeSelector: |
  #   sw-backend: "true"
  replication:
    enable: true
    collectionPattern: ""
  schedule: "*/7 * * * *"
  resources: null
  # balance all volumes among volume servers
  # ALL|EACH_COLLECTION|<collection_name>
  collection: ""